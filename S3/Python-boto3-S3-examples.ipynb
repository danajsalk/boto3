{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python boto3: S3 Linking Examples\n",
    "\n",
    "### This jupyter script shows some simple examples for beginner practice in interacting with AWS. More automated complex examples should potentially use another service like CloudFormation.\n",
    "\n",
    "### Before you begin this demo:\n",
    "\n",
    "* follow these directions from the boto3 configuration site to set up link between python boto3 and your aws account: \n",
    "    * https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#configuration\n",
    "\n",
    "1. `pip install boto3`\n",
    "2. configure boto3 to aws\n",
    "    * If you have the AWS CLI installed, then you can use it to configure your credentials file:\n",
    "    * `aws configure `\n",
    "    \n",
    "### Important Notes on following material:\n",
    "* **S3 bucket names are universal and MUST BE UNIQUE**\n",
    "* **BEST PRACTICE: MAKE S3 BUCKETS PRIVATE**\n",
    "```\n",
    "response = client.put_public_access_block(\n",
    "\t\t    Bucket=my_bucket,\n",
    "\t\t    PublicAccessBlockConfiguration={\n",
    "\t\t        'BlockPublicAcls': True,\n",
    "\t\t        'IgnorePublicAcls': True,\n",
    "\t\t        'BlockPublicPolicy': True,\n",
    "\t\t        'RestrictPublicBuckets': True\n",
    "\t\t    }\n",
    "\t\t)\n",
    " ```\n",
    "* When uploading files to S3, you have a 5 GB limit\n",
    "* Use Stubber for Testing\n",
    "\n",
    "### Response Syntax\n",
    "* https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObject.html\n",
    "\n",
    "Response Elements\n",
    "* If the action is successful, the service sends back an HTTP 204 response.Only returns header\n",
    "\n",
    "### Useful Links\n",
    "* Boto3 Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/migrations3.html\n",
    "* Boto3 Selecting Services: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/index.html\n",
    "* Boto3 Testing Stubber: https://botocore.amazonaws.com/v1/documentation/api/latest/reference/stubber.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "session = boto3.Session()\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Buckets - resource('s3')\n",
    "\n",
    "`s3 = boto3.resource('s3')`\n",
    "\n",
    "* Read bucket\n",
    "* Create bucket\n",
    "* Check if bucket exists\n",
    "* Delete Bucket\n",
    "* Read objects in buckets\n",
    "* Add objects to bucket\n",
    "* Delete objects in buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with buckets\n",
    "\n",
    "This section works with creating, reading, and deleting S3 buckets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read All S3 Buckets\n",
    "\n",
    "* `s3.buckets.all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket\n",
    "\n",
    "* `s3.create_bucket()`\n",
    "* #### BUCKETS ARE UNIVERSAL NAMES AND CANNOT BE THE SAME\n",
    "    * you'll get the error, \"Bucket name is already owned\"\n",
    "* You will have to create your own bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "s3.create_bucket(Bucket=my_bucket\n",
    "    , CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST PRACTICES: SET BUCKET TO PRIVATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.put_public_access_block(\n",
    "    Bucket=my_bucket,\n",
    "    PublicAccessBlockConfiguration={\n",
    "        'BlockPublicAcls': True,\n",
    "        'IgnorePublicAcls': True,\n",
    "        'BlockPublicPolicy': True,\n",
    "        'RestrictPublicBuckets': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if a bucket exists \n",
    "\n",
    "**Method** 1: Pull entire list of buckets and check to see if it exists\n",
    "* `s3.buckets.all()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    if my_bucket == bucket.name:\n",
    "        print(\"Bucket Exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: Use a bucket function\n",
    "* `bucket.creation_date`\n",
    "* it doesn't require ListBuckets which can be expensive\n",
    "* it doesn't require going down to the low-level client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "if bucket.creation_date:\n",
    "    print(\"The bucket exists, created:\", bucket.creation_date)\n",
    "else:\n",
    "    print(\"The bucket does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Bucket\n",
    "* `s3.Bucket(my_bucket).delete()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "\n",
    "s3.Bucket(my_bucket).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bucket unless it exists\n",
    "\n",
    "* `bucket.creation_date`\n",
    "* ` s3.create_bucket()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket('dana-demo-bucket')\n",
    "\n",
    "if bucket.creation_date:\n",
    "    print(\"The bucket already exists, created:\", bucket.creation_date)\n",
    "else:\n",
    "    s3.create_bucket(Bucket=my_bucket\n",
    "    , CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})\n",
    "    print(\"Created your bucket:\", my_bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Objects in Buckets\n",
    "\n",
    "This section works with the objects in a bucket using boto3 functions and the boto3client. This allows us to read dataframes into and out of buckets. Check to see if an object exists in a bucket and use object functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for objects in bucket\n",
    "\n",
    "* `bucket.objects.all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "print(len(list(bucket.objects.all())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Object to Bucket (like a dataframe as CSV)\n",
    "\n",
    "1. First I'm uploading a local example csv file as a dataframe\n",
    "2. Turn the df back into csv format \n",
    "3. Use put functionality to load into bucket\n",
    "\n",
    "\n",
    "`boto3.client()` https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#client\n",
    "\n",
    "* `client = boto3.client('s3')`\n",
    "* `client.put_object()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "\n",
    "df = pd.read_csv('My_Local_File.csv')\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer)\n",
    "\n",
    "bucket = s3.Bucket('dana-demo-bucket')\n",
    "client = boto3.client('s3')\n",
    "\n",
    "path = 'My_Local_File' + '.csv'\n",
    "client.put_object(\n",
    "              Body=csv_buffer.getvalue()\n",
    "            , Bucket=my_bucket\n",
    "            , Key=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add df in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('dana-demo-bucket')\n",
    "filename = 'My_Local_File'\n",
    "client = boto3.client('s3')\n",
    "    \n",
    "path = \"folder/\" + filename + '.csv' # ADD FOLDER TO PATH\n",
    "client.put_object(\n",
    "              Body=csv_buffer.getvalue()\n",
    "            , Bucket=my_bucket\n",
    "            , Key=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the Easy Way...\n",
    "### Add file using S3 client\n",
    "* s3.meta.client.upload_file()\n",
    "* https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "# 5 gig max?\n",
    "s3.meta.client.upload_file('hello_world.txt', my_bucket, 'hello.txt') \n",
    "s3.meta.client.upload_file('My_Local_File.csv', my_bucket, 'local_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send df to S3 Bucket Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_dataframe_to_S3(df, bucketName, path):\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "    \n",
    "    \n",
    "    print(\"WRITING to S3 bucket\", bucketName)\n",
    "\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer)\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        response = client.put_object(\n",
    "              Body=csv_buffer.getvalue()\n",
    "            , Bucket=bucketName\n",
    "            , Key=path)\n",
    "        print(\"SUCCESSFUL WRITE to the S3 BUCKET\")\n",
    "        print(\"YOUR FILES ARE HERE: \", path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (\"UNSUCCESSFUL write to S3 BUCKET \")\n",
    "        print(\"ERROR:\", e)\n",
    "\n",
    "#########################################################\n",
    "        \n",
    "from io import StringIO\n",
    "\n",
    "filename = 'My_Local_File'\n",
    "df = pd.read_csv(filename + '.csv')\n",
    "\n",
    "my_bucket='dana-demo-bucket'\n",
    "\n",
    "path = filename + '.csv'\n",
    "send_dataframe_to_S3(df, my_bucket, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 CSV to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method** 1: Direct link to a single document\n",
    "* `df = pd.read_csv(path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = filename + \".csv\"\n",
    "path = \"s3://\" + my_bucket + \"/\" + file\n",
    "print(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2:** key/body to get list of dataframes\n",
    "\n",
    "\n",
    "CSVs in S3 buckets are stored as bytes and need to be decoded to be read by the pands.read_csv() function. \n",
    "* `bucket.objects.all()`\n",
    "    * `obj.key`\n",
    "    * `obj.get()`\n",
    "* import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "df_list = []\n",
    "for obj in bucket.objects.all():\n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read() # body stored in bytes\n",
    "\n",
    "    csv_string = body.decode('utf-8')\n",
    "    df_list.append(pd.read_csv(StringIO(csv_string)))\n",
    "    \n",
    "print(len(df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ALL Objects from S3 Bucket\n",
    "\n",
    "* `bucket.objects.all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('dana-demo-bucket')\n",
    "\n",
    "obj_list = list(bucket.objects.all())\n",
    "if len(obj_list) > 0:\n",
    "    print(len(obj_list), \"Objects Exist\")\n",
    "else:\n",
    "    print(\"This bucket is empty\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print list of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print all object \"keys\" or names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "for file in bucket.objects.all():\n",
    "    print(file.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Object Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('dana-demo-bucket')\n",
    "\n",
    "file_to_find = 'My_Local_File.csv'\n",
    "for file in bucket.objects.all():\n",
    "    if file.key == file_to_find:\n",
    "        print(file_to_find, \"EXISTS in S3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Single Object in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "\n",
    "file_to_delete = 'My_Local_File.csv'\n",
    "obj = s3.Object(my_bucket, file_to_delete)\n",
    "obj.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up: Delete ALL Objects in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket='dana-demo-bucket'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "\n",
    "print(\"number objects: \", len(list(bucket.objects.all())))\n",
    "\n",
    "for obj in bucket.objects.all():\n",
    "    \n",
    "    ob = s3.Object(my_bucket,  obj.key)\n",
    "    ob.delete()\n",
    "    \n",
    "print(\"number objects: \", len(list(bucket.objects.all())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
